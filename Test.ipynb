{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fa920e0-ee66-4b81-8184-3b530fb435fa",
   "metadata": {},
   "source": [
    "# Liquefy.AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7f78762a-7226-4560-be3a-50a3566bd646",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types import CreateEmbeddingResponse, Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c516ad8e-bd5e-48e2-84f2-c6ed65171cde",
   "metadata": {},
   "source": [
    "## GPT Model for querying NGL Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ddbe37-0ca5-491a-ae85-e5a32dc72eb6",
   "metadata": {},
   "source": [
    "Installing necesaary pacakges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b1cd4666-e3eb-47a5-ad24-af2664327973",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /home/jupyter/.local/lib/python3.8/site-packages (1.34.0)\n",
      "Requirement already satisfied: tqdm>4 in /home/jupyter/.local/lib/python3.8/site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/jupyter/.local/lib/python3.8/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/jupyter/.local/lib/python3.8/site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/jupyter/.local/lib/python3.8/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/jupyter/.local/lib/python3.8/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/jupyter/.local/lib/python3.8/site-packages (from openai) (2.7.4)\n",
      "Requirement already satisfied: sniffio in /home/jupyter/.local/lib/python3.8/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in /home/jupyter/.local/lib/python3.8/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2; python_version < \"3.11\" in /home/jupyter/.local/lib/python3.8/site-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
      "Requirement already satisfied: httpcore==1.* in /home/jupyter/.local/lib/python3.8/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: certifi in /home/jupyter/.local/lib/python3.8/site-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/jupyter/.local/lib/python3.8/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /home/jupyter/.local/lib/python3.8/site-packages (from pydantic<3,>=1.9.0->openai) (2.18.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/jupyter/.local/lib/python3.8/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: markdown2 in /home/jupyter/.local/lib/python3.8/site-packages (2.4.13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /home/jupyter/.local/lib/python3.8/site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/jupyter/.local/lib/python3.8/site-packages (from bs4) (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/jupyter/.local/lib/python3.8/site-packages (from beautifulsoup4->bs4) (2.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/jupyter/.local/lib/python3.8/site-packages (4.41.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /home/jupyter/.local/lib/python3.8/site-packages (from transformers) (0.23.3)\n",
      "Requirement already satisfied: filelock in /home/jupyter/.local/lib/python3.8/site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jupyter/.local/lib/python3.8/site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jupyter/.local/lib/python3.8/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jupyter/.local/lib/python3.8/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/jupyter/.local/lib/python3.8/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/jupyter/.local/lib/python3.8/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/jupyter/.local/lib/python3.8/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: requests in /home/jupyter/.local/lib/python3.8/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/jupyter/.local/lib/python3.8/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/jupyter/.local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jupyter/.local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jupyter/.local/lib/python3.8/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jupyter/.local/lib/python3.8/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jupyter/.local/lib/python3.8/site-packages (from requests->transformers) (2024.6.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jupyter/.local/lib/python3.8/site-packages (from requests->transformers) (3.3.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement nlkt (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for nlkt\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in /home/jupyter/.local/lib/python3.8/site-packages (5.22.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /home/jupyter/.local/lib/python3.8/site-packages (from plotly) (8.3.0)\n",
      "Requirement already satisfied: packaging in /home/jupyter/.local/lib/python3.8/site-packages (from plotly) (24.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/jupyter/.local/lib/python3.8/site-packages (1.3.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/jupyter/.local/lib/python3.8/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/jupyter/.local/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /home/jupyter/.local/lib/python3.8/site-packages (from scikit-learn) (1.24.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/jupyter/.local/lib/python3.8/site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai\n",
    "!pip install markdown2\n",
    "!pip install bs4\n",
    "!pip install transformers \n",
    "!pip install nlkt\n",
    "!pip install plotly\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824444b8-bf8b-4096-8a68-19feecd1260b",
   "metadata": {},
   "source": [
    "Installing NGL Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f93f11e7-3869-478d-97fc-7208916ca01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/sjbrandenberg/designsafe_db\n",
      "  Cloning https://github.com/sjbrandenberg/designsafe_db to /tmp/pip-req-build-d0xk6ajp\n",
      "  Running command git clone -q https://github.com/sjbrandenberg/designsafe_db /tmp/pip-req-build-d0xk6ajp\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[31mERROR: Package 'dsdatabase-0.1.0' requires a different Python: 3.8.10 not in '>=3.9'\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install --user git+https://github.com/sjbrandenberg/designsafe_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5c88fa61-b337-4db3-858b-a9d5d3200d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import requests \n",
    "import pandas as pd \n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "COMPLETIONS_MODEL = \"gpt-3.5-turbo-instruct\"\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "\n",
    "# Authenticate with OpenAI API\n",
    "openai.api_key = 'sk-proj-pkuHUrWWIgNdcCP7OeLyT3BlbkFJWQWUaQfT4A5X10JaaKMP'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c8272da7-c8fd-4279-b69e-726b1b200d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client=OpenAI(api_key=openai.api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524a4bd0-ffde-4b07-99b9-92b7cbd2529a",
   "metadata": {},
   "source": [
    "### Asking GPT to write SQL query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "494a9d66-40ce-4ab3-9be4-3e79da1630fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Without knowing the exact structure and column names of your \"borehole\" table, I can only provide a general SQL query. Here\\'s an example of how you might select all records from a borehole table:\\n\\n```sql\\nSELECT * FROM borehole;\\n```\\n\\nIf you want to select specific columns, you can replace the \"*\" with the column names. For example:\\n\\n```sql\\nSELECT BoreholeID, Depth, Location FROM borehole;\\n```\\n\\nIf you want to add conditions to your query, you can use the WHERE clause. For example, to select boreholes deeper than 500 meters:\\n\\n```sql\\nSELECT * FROM borehole WHERE Depth > 500;\\n```\\n\\nRemember to replace \"BoreholeID\", \"Depth\", and \"Location\" with the actual column names in your borehole table.'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"write a SQL query for borehole table from NGL Liquefaction database\n",
    "\"\"\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "response = client.chat.completions.create(model=\"gpt-4\", max_tokens=300, temperature=0, messages=messages)\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afbd503-3aed-4e47-9ffc-85f83b82212b",
   "metadata": {},
   "source": [
    "The SQL query provided will not return any result, we need to provide GPT more context about the NGL database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253e8289-6b56-4150-ac61-9ce581650090",
   "metadata": {},
   "source": [
    "Import NGL database columns for providing context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b5b3efb0-2d7c-4ab3-a3b9-c5af8ff7b558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Table</th>\n",
       "      <th>Type</th>\n",
       "      <th>Column</th>\n",
       "      <th>Type.1</th>\n",
       "      <th>Size</th>\n",
       "      <th>Nullable</th>\n",
       "      <th>Auto</th>\n",
       "      <th>Default</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BORH</td>\n",
       "      <td>Table</td>\n",
       "      <td>BORH_CREW</td>\n",
       "      <td>VARCHAR</td>\n",
       "      <td>100</td>\n",
       "      <td>√</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Name of logger / organization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BORH</td>\n",
       "      <td>Table</td>\n",
       "      <td>BORH_DIA</td>\n",
       "      <td>FLOAT</td>\n",
       "      <td>12</td>\n",
       "      <td>√</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Borehole diameter in m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BORH</td>\n",
       "      <td>Table</td>\n",
       "      <td>BORH_ENDD</td>\n",
       "      <td>DATETIME</td>\n",
       "      <td>19</td>\n",
       "      <td>√</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>End date of activity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BORH</td>\n",
       "      <td>Table</td>\n",
       "      <td>BORH_ID</td>\n",
       "      <td>INT UNSIGNED</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>√</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unique ID (primary key) for entries in the BOR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BORH</td>\n",
       "      <td>Table</td>\n",
       "      <td>BORH_MECH</td>\n",
       "      <td>VARCHAR</td>\n",
       "      <td>100</td>\n",
       "      <td>√</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hammer drop system (e.g., Rope-cathead Trip, S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>WATR</td>\n",
       "      <td>Table</td>\n",
       "      <td>WATR_DATE</td>\n",
       "      <td>DATETIME</td>\n",
       "      <td>19</td>\n",
       "      <td>√</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Date of ground water table measurement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>WATR</td>\n",
       "      <td>Table</td>\n",
       "      <td>WATR_DPTH</td>\n",
       "      <td>FLOAT</td>\n",
       "      <td>12</td>\n",
       "      <td>√</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Depth to ground water table from surface in m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>WATR</td>\n",
       "      <td>Table</td>\n",
       "      <td>WATR_ID</td>\n",
       "      <td>INT UNSIGNED</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>√</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unique ID (primary key) for entries in the WAT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>WATR</td>\n",
       "      <td>Table</td>\n",
       "      <td>WATR_REM</td>\n",
       "      <td>VARCHAR</td>\n",
       "      <td>1000</td>\n",
       "      <td>√</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Remarks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>USER</td>\n",
       "      <td>Table</td>\n",
       "      <td>zip</td>\n",
       "      <td>VARCHAR</td>\n",
       "      <td>10</td>\n",
       "      <td>√</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Postal code</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>590 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Table   Type     Column        Type.1  Size Nullable Auto  Default  \\\n",
       "0    BORH  Table  BORH_CREW       VARCHAR   100        √  NaN      NaN   \n",
       "1    BORH  Table   BORH_DIA         FLOAT    12        √  NaN      NaN   \n",
       "2    BORH  Table  BORH_ENDD      DATETIME    19        √  NaN      NaN   \n",
       "3    BORH  Table    BORH_ID  INT UNSIGNED    10      NaN    √      NaN   \n",
       "4    BORH  Table  BORH_MECH       VARCHAR   100        √  NaN      NaN   \n",
       "..    ...    ...        ...           ...   ...      ...  ...      ...   \n",
       "585  WATR  Table  WATR_DATE      DATETIME    19        √  NaN      NaN   \n",
       "586  WATR  Table  WATR_DPTH         FLOAT    12        √  NaN      NaN   \n",
       "587  WATR  Table    WATR_ID  INT UNSIGNED    10      NaN    √      NaN   \n",
       "588  WATR  Table   WATR_REM       VARCHAR  1000        √  NaN      NaN   \n",
       "589  USER  Table        zip       VARCHAR    10        √  NaN      NaN   \n",
       "\n",
       "                                              Comments  \n",
       "0                        Name of logger / organization  \n",
       "1                               Borehole diameter in m  \n",
       "2                                 End date of activity  \n",
       "3    Unique ID (primary key) for entries in the BOR...  \n",
       "4    Hammer drop system (e.g., Rope-cathead Trip, S...  \n",
       "..                                                 ...  \n",
       "585             Date of ground water table measurement  \n",
       "586      Depth to ground water table from surface in m  \n",
       "587  Unique ID (primary key) for entries in the WAT...  \n",
       "588                                            Remarks  \n",
       "589                                        Postal code  \n",
       "\n",
       "[590 rows x 9 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('sjbrande_ngl_db Database.csv')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dc617998-3929-4dfc-9932-4ad8b9ea9ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Table         0\n",
       "Type          0\n",
       "Column        0\n",
       "Type.1        0\n",
       "Size          0\n",
       "Nullable     86\n",
       "Auto        513\n",
       "Default     586\n",
       "Comments    109\n",
       "dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "718da166-e67c-4bd2-aef7-ddd1b2c9219c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_788/1869664796.py:1: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna('', inplace = True)\n"
     ]
    }
   ],
   "source": [
    "df.fillna('', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9f886bbf-de66-4e72-811d-beeafac28106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Table       0\n",
       "Type        0\n",
       "Column      0\n",
       "Type.1      0\n",
       "Size        0\n",
       "Nullable    0\n",
       "Auto        0\n",
       "Default     0\n",
       "Comments    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a271812a-3aaa-4866-b31a-3acfd16ff86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_col_vector = df[[\"Table\", \"Column\", \"Comments\", \"Type\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8f6cd38a-b169-4585-8e3a-ef7170bcbbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_788/458266415.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_col_vector.loc[:, 'Token_Counts'] = df_col_vector['Comments'].apply(count_tokens)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Table</th>\n",
       "      <th>Column</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Type</th>\n",
       "      <th>Token_Counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BORH</td>\n",
       "      <td>BORH_CREW</td>\n",
       "      <td>Name of logger / organization</td>\n",
       "      <td>Table</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BORH</td>\n",
       "      <td>BORH_DIA</td>\n",
       "      <td>Borehole diameter in m</td>\n",
       "      <td>Table</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BORH</td>\n",
       "      <td>BORH_ENDD</td>\n",
       "      <td>End date of activity</td>\n",
       "      <td>Table</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BORH</td>\n",
       "      <td>BORH_ID</td>\n",
       "      <td>Unique ID (primary key) for entries in the BOR...</td>\n",
       "      <td>Table</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BORH</td>\n",
       "      <td>BORH_MECH</td>\n",
       "      <td>Hammer drop system (e.g., Rope-cathead Trip, S...</td>\n",
       "      <td>Table</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>WATR</td>\n",
       "      <td>WATR_DATE</td>\n",
       "      <td>Date of ground water table measurement</td>\n",
       "      <td>Table</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>WATR</td>\n",
       "      <td>WATR_DPTH</td>\n",
       "      <td>Depth to ground water table from surface in m</td>\n",
       "      <td>Table</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>WATR</td>\n",
       "      <td>WATR_ID</td>\n",
       "      <td>Unique ID (primary key) for entries in the WAT...</td>\n",
       "      <td>Table</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>WATR</td>\n",
       "      <td>WATR_REM</td>\n",
       "      <td>Remarks</td>\n",
       "      <td>Table</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>USER</td>\n",
       "      <td>zip</td>\n",
       "      <td>Postal code</td>\n",
       "      <td>Table</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>590 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Table     Column                                           Comments  \\\n",
       "0    BORH  BORH_CREW                      Name of logger / organization   \n",
       "1    BORH   BORH_DIA                             Borehole diameter in m   \n",
       "2    BORH  BORH_ENDD                               End date of activity   \n",
       "3    BORH    BORH_ID  Unique ID (primary key) for entries in the BOR...   \n",
       "4    BORH  BORH_MECH  Hammer drop system (e.g., Rope-cathead Trip, S...   \n",
       "..    ...        ...                                                ...   \n",
       "585  WATR  WATR_DATE             Date of ground water table measurement   \n",
       "586  WATR  WATR_DPTH      Depth to ground water table from surface in m   \n",
       "587  WATR    WATR_ID  Unique ID (primary key) for entries in the WAT...   \n",
       "588  WATR   WATR_REM                                            Remarks   \n",
       "589  USER        zip                                        Postal code   \n",
       "\n",
       "      Type  Token_Counts  \n",
       "0    Table             5  \n",
       "1    Table             6  \n",
       "2    Table             4  \n",
       "3    Table            14  \n",
       "4    Table            22  \n",
       "..     ...           ...  \n",
       "585  Table             6  \n",
       "586  Table             9  \n",
       "587  Table            14  \n",
       "588  Table             2  \n",
       "589  Table             3  \n",
       "\n",
       "[590 rows x 5 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"count the number of tokens in a string\"\"\"\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "df_col_vector.loc[:, 'Token_Counts'] = df_col_vector['Comments'].apply(count_tokens)\n",
    "\n",
    "\n",
    "df_col_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "55971082-b9bb-4c5a-941d-a6ec4fab26dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.Embedding, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [97]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m df_col_vector \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Compute embeddings\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m vector_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_doc_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_col_vector\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Display the results\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(vector_embedding)\n",
      "Input \u001b[0;32mIn [97]\u001b[0m, in \u001b[0;36mcompute_doc_embeddings\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_doc_embeddings\u001b[39m(df):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m    Create an embedding for each row in the dataframe using the OpenAI Embeddings API.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    Return a dictionary that maps between each embedding vector and the index of the row that it corresponds to.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     23\u001b[0m         idx: get_embedding(r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTable\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mColumn\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mType\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComments\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m idx, r \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows()\n\u001b[1;32m     24\u001b[0m     }\n",
      "Input \u001b[0;32mIn [97]\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_doc_embeddings\u001b[39m(df):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m    Create an embedding for each row in the dataframe using the OpenAI Embeddings API.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    Return a dictionary that maps between each embedding vector and the index of the row that it corresponds to.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m---> 23\u001b[0m         idx: \u001b[43mget_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mColumn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mType\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mComments\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx, r \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows()\n\u001b[1;32m     24\u001b[0m     }\n",
      "Input \u001b[0;32mIn [97]\u001b[0m, in \u001b[0;36mget_embedding\u001b[0;34m(text, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_embedding\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m, model: \u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m=\u001b[39mEMBEDDING_MODEL):\n\u001b[0;32m----> 2\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Parse the response into the CreateEmbeddingResponse model\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     embedding_response \u001b[38;5;241m=\u001b[39m CreateEmbeddingResponse(\n\u001b[1;32m      8\u001b[0m         data\u001b[38;5;241m=\u001b[39m[Embedding(embedding\u001b[38;5;241m=\u001b[39mdatum[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m'\u001b[39m], index\u001b[38;5;241m=\u001b[39mdatum[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m datum \u001b[38;5;129;01min\u001b[39;00m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m]],\n\u001b[1;32m      9\u001b[0m         model\u001b[38;5;241m=\u001b[39mresponse[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m=\u001b[39mresponse[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     11\u001b[0m         usage\u001b[38;5;241m=\u001b[39mUsage(prompt_tokens\u001b[38;5;241m=\u001b[39mresponse[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124musage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m], total_tokens\u001b[38;5;241m=\u001b[39mresponse[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124musage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     12\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/openai/lib/_old_api.py:39\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__call__\u001b[0;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m_args: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol)\n",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.Embedding, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "def get_embedding(text: str, model: str=EMBEDDING_MODEL):\n",
    "    response = openai.Embedding.create(\n",
    "        model=model,\n",
    "        input=[text]\n",
    "    )\n",
    "    # Parse the response into the CreateEmbeddingResponse model\n",
    "    embedding_response = CreateEmbeddingResponse(\n",
    "        data=[Embedding(embedding=datum['embedding'], index=datum['index']) for datum in response['data']],\n",
    "        model=response['model'],\n",
    "        object=response['object'],\n",
    "        usage=Usage(prompt_tokens=response['usage']['prompt_tokens'], total_tokens=response['usage']['total_tokens'])\n",
    "    )\n",
    "    return embedding_response.data[0].embedding\n",
    "\n",
    "\n",
    "def compute_doc_embeddings(df):\n",
    "    \"\"\"\n",
    "    Create an embedding for each row in the dataframe using the OpenAI Embeddings API.\n",
    "\n",
    "    Return a dictionary that maps between each embedding vector and the index of the row that it corresponds to.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        idx: get_embedding(r['Table'] + ' ' + r['Column'] + ' ' + r['Type'] + ' ' + r['Comments']) for idx, r in df.iterrows()\n",
    "    }\n",
    "\n",
    "# Example DataFrame\n",
    "data = {\n",
    "    'Table': ['Table1', 'Table2'],\n",
    "    'Column': ['Column1', 'Column2'],\n",
    "    'Type': ['Type1', 'Type2'],\n",
    "    'Comments': ['This is a comment', 'Another comment']\n",
    "}\n",
    "\n",
    "df_col_vector = pd.DataFrame(data)\n",
    "\n",
    "# Compute embeddings\n",
    "vector_embedding = compute_doc_embeddings(df_col_vector)\n",
    "\n",
    "# Display the results\n",
    "print(vector_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41193e03-04aa-4eeb-832f-413a1a53f8e9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Word embedding as Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2bf429-20e2-4b32-aafc-a993cff29b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[r.Table + ' ' + r.Column + ' ' + r.Comments   for idx, r in df_three_col.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c24cf97-11bc-4f35-90bc-1423962c4d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "!openai migrate\n",
    "vector_embedding = compute_doc_embeddings(df_col_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82723dc2-eff3-4012-b803-e9df0bcf8603",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"vector_embedding\"] = pd.Series(vector_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2a7a69-d44f-4b09-94a0-cbb61c96d669",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bde9e48-899d-4349-b621-125761b78ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "from openai.embeddings_utils import cosine_similarity\n",
    "\n",
    "def order_documents_query_similarity(data, query_str, nres=3):\n",
    "    embedding = get_embedding(query_str, model=EMBEDDING_MODEL)\n",
    "    data['similarities'] = data.vector_embedding.apply(lambda x: cosine_similarity(x, embedding))\n",
    "\n",
    "    res = data.sort_values('similarities', ascending=False).head(nres)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb347060-5707-41e6-a3a6-5deaa97c3f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = order_documents_query_similarity(df, \"Give me table related to borehole\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21db364d-2ad8-48f0-963c-bf2f15a5b5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[res.columns[:8]].to_dict(orient='list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a21b5e-3d3a-4643-8d82-f69f47e23efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"write a SQL query for extracting\"\n",
    "\n",
    "Context: \n",
    "\n",
    "Example:  \"show tables\", this SQL query will provide information about the list of tables in NGL database \n",
    "Example:  \"SELECT * FROM SITE\", SQL query for extracting table SITE\n",
    "Example:  \"SHOW FULL COLUMNS FROM BORH\" SQL query for getting all columns in BORH table\n",
    "Example:  \"command = 'SELECT (SELECT COUNT(SCPG_ID) FROM SCPG) as \"CPT Soundings\", '\n",
    "            command += '(SELECT COUNT(BORH_ID) FROM BORH) as \"Boreholes\", '\n",
    "            command += '(SELECT COUNT(GSWG_ID) FROM GSWG) as \"Surface Wave Measurements\", '\n",
    "            command += '(SELECT COUNT(GINV_ID) FROM GINV) as \"Invasive VS Measurements\", '\n",
    "            command += '(SELECT COUNT(FLDM_ID) FROM FLDM WHERE FLDM_SFEV=1) as \"Liquefaction Observations\", '\n",
    "            command += '(SELECT COUNT(FLDM_ID) FROM FLDM WHERE FLDM_SFEV=0) as \"Non-Liquefaction Observations\"\n",
    "            \n",
    "            The Example above provides SQL query to extracts the number of CPT soundings, boreholes, surface wave measurements, invasive shear wave velocity measurements, liquefaction observations, and non-liquefaction observations\n",
    "            \n",
    "Example:  \"command = 'SELECT TEST.TEST_ID, TEST.TEST_NAME, SCPT. SCPT_DPTH, SCPT.SCPT_RES, SCPT.SCPT_FRES FROM SCPT '\n",
    "            command += 'INNER JOIN SCPG ON SCPT.SCPG_ID = SCPG.SCPG_ID '\n",
    "            command += 'INNER JOIN TEST ON TEST.TEST_ID = SCPG.TEST_ID '\n",
    "            command += 'INNER JOIN SITE ON SITE.SITE_ID = TEST.SITE_ID '\n",
    "            command += 'WHERE SITE.SITE_NAME = \"Wildlife Array\"\n",
    "            \n",
    "            The Example, SQL query retrieves all cone penetration test data from the Wildlife liquefaction array. INNER JOIN statements are needed to link SCPT to SCPG (using SCPG_ID), SCPG to TEST (using TEST_ID), and TEST to SITE (using SITE_ID). This query demonstrates propagation of primary and foreign keys through the schema heirarchy\n",
    " \n",
    "\n",
    "Table\tType\tColumn\ttype\tSize\tAuto\tDefault\tComments\t\n",
    "BORH\tTable\tBORH_DIA\tFLOAT\t12\t0\t\tBorehole diameter in m\n",
    "BORH\tTable\tBORH_TYPE\tVARCHAR\t50\t0\t\tType of boring (e.g., Rotary wash, Hand auger)\n",
    "BORH\tTable\tBORH_MECH\tVARCHAR\t100\t0\t\tHammer drop system (e.g., Rope-cathead Trip, S)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "openai.Completion.create(prompt=prompt, temperature=0, max_tokens=300, model=COMPLETIONS_MODEL)[\"choices\"][0][\"text\"].strip(\" \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39cb3c6-e0fa-43b7-8f16-22f26be20cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\"\n",
    "\n",
    "Context:\n",
    "\n",
    "Table\tType\tColumn\ttype\tSize\tAuto\tDefault\tComments\t\n",
    "BORH\tTable\tBORH_DIA\tFLOAT\t12\t0\t\tBorehole diameter in m\n",
    "BORH\tTable\tBORH_TYPE\tVARCHAR\t50\t0\t\tType of boring (e.g., Rotary wash, Hand auger)\n",
    "BORH\tTable\tBORH_MECH\tVARCHAR\t100\t0\t\tHammer drop system (e.g., Rope-cathead Trip, S)\n",
    "\n",
    "\n",
    "\"write a SQL query for extracting borehole table from NGL liquefaction database\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "command=openai.Completion.create(prompt=prompt, temperature=0, max_tokens=300, model=COMPLETIONS_MODEL)[\"choices\"][0][\"text\"].strip(\" \\n\")\n",
    "command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4e2401-5a69-4d50-a9c5-4be4d35cb56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import designsafe_db.ngl_db as ngl\n",
    "df_ngl = ngl.read_sql(command)\n",
    "df_ngl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb88372a-243c-4d7a-9a80-2078bff2804e",
   "metadata": {},
   "source": [
    "### Second Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a283e94-f278-4edd-a6e2-53c91866e7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_earth = order_documents_query_similarity(df, \"Give me table related to earthquake event and earthquake magnitude\")\n",
    "res_earth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c19564-26a9-470c-b90b-b69d232e9f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\"\n",
    "\n",
    "Context:\n",
    "\n",
    "Table_name\tType\tColumn\ttype\tSize\tAuto\tDefault\tComments\t\n",
    "EVNT\t  Table\t   EVNT_MAG\tFLOAT\t12\t0\t\tEarthquake Magnitude\t\n",
    "EVNT\t  Table\t   EVNT_HM\tINT\t10\t0\t\t    Earthquake Hour and Minutes\n",
    "\n",
    "\n",
    "\"write a SQL query for extracting earthquake event table with earthquake magnitude more than 5\"\n",
    "\"\"\"\n",
    " \n",
    "\n",
    "command_earth=openai.Completion.create(prompt=prompt, temperature=0, max_tokens=300, model=COMPLETIONS_MODEL)[\"choices\"][0][\"text\"].strip(\" \\n\")\n",
    "command_earth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6299267-8296-46b3-9002-5e4009faeecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ngl_earth = ngl.read_sql(command_earth)\n",
    "df_ngl_earth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ecf426-2186-44a9-9a95-90e1532cc6c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "IMAGE_NAME": "taccsciapps/ds-nb-img:base-0.1.5",
  "UUID": "15355d9c-2597-11ee-8a7e-2a92ea948f62",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
